{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('opencv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "743ec167446bbfdda66c1cab5296ceef60a46f4eacfb9cbdd7f3823da45230eb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.contours import sort_contours\n",
    "\n",
    "from src import fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins du projet\n",
    "DATA_ROOT = 'data/'\n",
    "MODELE_PATH = DATA_ROOT + 'model.h5'\n",
    "TRAINING_PATH = DATA_ROOT + 'train'\n",
    "TESTING_PATH = DATA_ROOT + 'test'\n",
    "\n",
    "# Autres paramètres\n",
    "batch_size = 32\n",
    "num_classes = 100\n",
    "epochs = 35\n",
    "img_size = 28\n",
    "input_shape = (img_size, img_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonctions.import_fichier_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonctions.import_fichier_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 11600 images belonging to 100 classes.\n",
      "Found 48400 images belonging to 100 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### ImageDataGenerator génère des lots de données d'image vectorielles, convertissant les coefficients RVB compris entre 0 et 255 en valeurs cibles comprises entre 0 et 1 par mise à l'échelle avec un facteur de 1/255 à l' aide de la remise à l' échelle ###\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    ### shear_range est utilisé pour appliquer de manière aléatoire des transformations de cisaillement ###\n",
    "    shear_range = 0.2,\n",
    "    ### zoom_range est utilisé pour zoomer aléatoirement à l'intérieur des images ###           \n",
    "    zoom_range = 0.2,\n",
    "    ### horizontal_flip est utilisé pour retourner au hasard la moitié des images horizontalement ###            \n",
    "    horizontal_flip = True\n",
    ")      \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "\n",
    "### J'importe les images une par une à partir des répertoires en utilisant .flow_from_directory et y appliquons ImageDataGenerator ###\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    ### Choix de mon repertoire ###\n",
    "    directory = TRAINING_PATH,\n",
    "    ### Je converti les images de leur taille d'origine à notre target_size ###                    \n",
    "    target_size = (img_size,img_size),\n",
    "    ### Nombre batch_size qui fait référence au nombre d'exemples d'entraînement utilisés dans une itération ###                                      \n",
    "    batch_size = batch_size,\n",
    "    ### Je definis le class_mode sur \"catégorical\" indiquant que nous avons plusieurs classes (a à z) à prédire ###          \n",
    "    class_mode = \"categorical\",\n",
    "    ### Je choisis le color_mode \"grayscale\", indiquant que nous trvaillons sur une image en noir et blanc\n",
    "    color_mode = \"grayscale\"                                \n",
    "\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = TESTING_PATH,\n",
    "    target_size = (img_size,img_size),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = \"categorical\",\n",
    "    color_mode = \"grayscale\"\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Simplon\\anaconda3\\envs\\opencv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 124,996\n",
      "Trainable params: 124,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Création d'un modèle séquentiel qui permet de définir l'architecture CNN couche par couche à l'aide de la fonction .add .Nous ajoutons d'abord une couche de convolution avec 32 filtres de taille 3X3 sur les images d'entrée et la passons à travers la fonction d'activation 'relu'.Nous effectuons ensuite des opérations MaxPooling en utilisant un pool de taille 2X2 ###\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape = input_shape, activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "### Ces couches sont ensuite répétées à nouveau pour améliorer les performances du modèle ###\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "### Enfin, nous aplatissons notre matrice résultante et la passons à travers une couche dense composée de 128 nœuds. Celui-ci est ensuite connecté à la couche de sortie constituée de 26 nœuds, chaque nœud représentant un alphabet ###\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation = \"relu\"))\n",
    "model.add(Dense(units = 100, activation = \"softmax\"))            ### Activation softmax qui convertit les scores en une distribution de probabilité normalisée, et                                                                   le nœud avec la probabilité la plus élevée est sélectionné comme sortie ###\n",
    "\n",
    "### Une fois notre architecture CNN définie, nous compilons le modèle à l'aide de l'optimiseur Adam ###\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Simplon\\anaconda3\\envs\\opencv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/35\n",
      "32/32 [==============================] - 22s 675ms/step - loss: 4.5336 - accuracy: 0.0498 - val_loss: 4.6909 - val_accuracy: 9.7656e-04\n",
      "Epoch 2/35\n",
      "32/32 [==============================] - 16s 505ms/step - loss: 4.4959 - accuracy: 0.0537 - val_loss: 4.6993 - val_accuracy: 9.7656e-04\n",
      "Epoch 3/35\n",
      "32/32 [==============================] - 16s 514ms/step - loss: 4.5009 - accuracy: 0.0469 - val_loss: 4.6226 - val_accuracy: 0.0039\n",
      "Epoch 4/35\n",
      "32/32 [==============================] - 15s 483ms/step - loss: 4.5197 - accuracy: 0.0596 - val_loss: 4.7612 - val_accuracy: 0.0029\n",
      "Epoch 5/35\n",
      "32/32 [==============================] - 16s 489ms/step - loss: 4.4662 - accuracy: 0.0645 - val_loss: 4.7022 - val_accuracy: 0.0029\n",
      "Epoch 6/35\n",
      "32/32 [==============================] - 17s 527ms/step - loss: 4.4752 - accuracy: 0.0791 - val_loss: 4.6613 - val_accuracy: 0.0020\n",
      "Epoch 7/35\n",
      "32/32 [==============================] - 16s 504ms/step - loss: 4.4412 - accuracy: 0.0723 - val_loss: 4.8222 - val_accuracy: 0.0020\n",
      "Epoch 8/35\n",
      "32/32 [==============================] - 16s 485ms/step - loss: 4.4188 - accuracy: 0.0833 - val_loss: 4.5440 - val_accuracy: 0.0039\n",
      "Epoch 9/35\n",
      "32/32 [==============================] - 16s 489ms/step - loss: 4.3592 - accuracy: 0.0850 - val_loss: 4.6645 - val_accuracy: 0.0029\n",
      "Epoch 10/35\n",
      "32/32 [==============================] - 16s 495ms/step - loss: 4.3648 - accuracy: 0.0840 - val_loss: 4.6460 - val_accuracy: 0.0049\n",
      "Epoch 11/35\n",
      "32/32 [==============================] - 17s 533ms/step - loss: 4.3378 - accuracy: 0.1025 - val_loss: 4.7191 - val_accuracy: 0.0020\n",
      "Epoch 12/35\n",
      "32/32 [==============================] - 11s 358ms/step - loss: 4.2971 - accuracy: 0.0830 - val_loss: 4.7581 - val_accuracy: 0.0029\n",
      "Epoch 13/35\n",
      "32/32 [==============================] - 12s 361ms/step - loss: 4.3321 - accuracy: 0.0791 - val_loss: 4.4813 - val_accuracy: 0.0088\n",
      "Epoch 14/35\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 4.2287 - accuracy: 0.1062 - val_loss: 4.6391 - val_accuracy: 0.0059\n",
      "Epoch 15/35\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 4.1597 - accuracy: 0.1113 - val_loss: 4.6097 - val_accuracy: 0.0137\n",
      "Epoch 16/35\n",
      "32/32 [==============================] - 12s 383ms/step - loss: 4.1662 - accuracy: 0.0996 - val_loss: 4.4789 - val_accuracy: 0.0205\n",
      "Epoch 17/35\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 4.2057 - accuracy: 0.1084 - val_loss: 4.4388 - val_accuracy: 0.0098\n",
      "Epoch 18/35\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 4.1435 - accuracy: 0.1104 - val_loss: 4.3699 - val_accuracy: 0.0127\n",
      "Epoch 19/35\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 4.1214 - accuracy: 0.1045 - val_loss: 4.2100 - val_accuracy: 0.0254\n",
      "Epoch 20/35\n",
      "32/32 [==============================] - 12s 361ms/step - loss: 4.1152 - accuracy: 0.0986 - val_loss: 4.3918 - val_accuracy: 0.0205\n",
      "Epoch 21/35\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 4.0816 - accuracy: 0.1299 - val_loss: 4.5679 - val_accuracy: 0.0303\n",
      "Epoch 22/35\n",
      "32/32 [==============================] - 11s 358ms/step - loss: 4.1193 - accuracy: 0.1074 - val_loss: 4.4192 - val_accuracy: 0.0312\n",
      "Epoch 23/35\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 4.0704 - accuracy: 0.1309 - val_loss: 4.1637 - val_accuracy: 0.0283\n",
      "Epoch 24/35\n",
      "32/32 [==============================] - 11s 359ms/step - loss: 4.0269 - accuracy: 0.1289 - val_loss: 4.3948 - val_accuracy: 0.0234\n",
      "Epoch 25/35\n",
      "32/32 [==============================] - 12s 371ms/step - loss: 4.0861 - accuracy: 0.1141 - val_loss: 4.4227 - val_accuracy: 0.0361\n",
      "Epoch 26/35\n",
      "32/32 [==============================] - 11s 359ms/step - loss: 3.9643 - accuracy: 0.1377 - val_loss: 4.0104 - val_accuracy: 0.0400\n",
      "Epoch 27/35\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 3.9835 - accuracy: 0.1191 - val_loss: 4.5708 - val_accuracy: 0.0332\n",
      "Epoch 28/35\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 3.9826 - accuracy: 0.1211 - val_loss: 4.3338 - val_accuracy: 0.0293\n",
      "Epoch 29/35\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 3.8863 - accuracy: 0.1367 - val_loss: 4.3758 - val_accuracy: 0.0469\n",
      "Epoch 30/35\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 4.0049 - accuracy: 0.1260 - val_loss: 4.2424 - val_accuracy: 0.0469\n",
      "Epoch 31/35\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 3.9554 - accuracy: 0.1367 - val_loss: 4.3757 - val_accuracy: 0.0420\n",
      "Epoch 32/35\n",
      "32/32 [==============================] - 11s 357ms/step - loss: 3.9048 - accuracy: 0.1484 - val_loss: 4.0221 - val_accuracy: 0.0459\n",
      "Epoch 33/35\n",
      "32/32 [==============================] - 11s 359ms/step - loss: 3.9023 - accuracy: 0.1484 - val_loss: 4.2648 - val_accuracy: 0.0791\n",
      "Epoch 34/35\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 3.8795 - accuracy: 0.1582 - val_loss: 4.2409 - val_accuracy: 0.0518\n",
      "Epoch 35/35\n",
      "32/32 [==============================] - 12s 378ms/step - loss: 3.8315 - accuracy: 0.1494 - val_loss: 4.2967 - val_accuracy: 0.0596\n",
      "Test de perte: 4.095990180969238\n",
      "Test de précision: 0.1515517234802246\n",
      "Enregistrement du modèle...\n",
      "Modèle enregistré!\n"
     ]
    }
   ],
   "source": [
    "### Je décide de créer 25 répétitions, et j'ai X2 les steps_per_epoch pour augmenter notre précision ###\n",
    "\n",
    "entrainement = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = batch_size\n",
    ")\n",
    "\n",
    "score = model.evaluate(train_generator, verbose=0)\n",
    "print(\"Test de perte:\", score[0])\n",
    "print(\"Test de précision:\", score[1])\n",
    "print(\"Enregistrement du modèle...\")\n",
    "model.save(MODELE_PATH)\n",
    "print(\"Modèle enregistré!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}